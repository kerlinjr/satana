{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Lets pretend I'm a HVAC repair company looking for clients to offer my services. I'd like to target my effort on customers that are most likely to use my services. I have access to aerial imagery of the roofs of houses in a city. I'd like to use this imagery to identify houses that have HVAC units on the roof that may be leaking. I have a set of 150 images labeled via spreadsheet in the following manner: \n",
    "\n",
    "'row_number' Images are numebred from 1 to 150  \n",
    "'image_id' - a unique identifier for each image, ending ing in .jpeg, referencing a file  \n",
    "'address' - The address of the house in the image  \n",
    "'ac_present' - Whether or not an air conditioner is present on the roof of the house (Yes or No), followed by the number of air conditioners present in '()'  \n",
    "'ac_leaking' - Whether or not the air conditioner is leaking (Yes, No or Unknown)  \n",
    "'ac_discolored' - Whether or not the roof around any AC unit is discolored (Black, White, No, Unknown). Also may be both black and white, comma separated: \"black, white\" OR \"white, black\"  \n",
    "'comments' - Any additional comments about the image from the labeler(s)  \n",
    "\n",
    "In the images, the target house is bounded by an orange square.  \n",
    "\n",
    "This will serve as my ground truth data. I'd like to build a model that can predict whether or not an air conditioner is leaking based on the aerial imagery of the roof of a house.\n",
    "\n",
    "Step 1: Preprocessing the data : Load the data labels spreadsheet from the folder /data/labels/. The optional \"number of air conditioners part in \"()\" should be split off and moved to a new column called num_ac, with all nulls set to 0.  Load in the images listed in image_id from the folder /data/images/ac_150_images/ in order to crop them around the orange bounding box. Output the images to a folder called 'ac_150_images_cropped'. The file names should be the 'image_id' from the spreadsheet, but with the .jpeg extension replaced with _cropped.jpeg. For example, an image with 'image_id' 56E8920C-C46B-4CC9-96CC-178AE00230D3.jpeg should be output as 56E8920C-C46B-4CC9-96CC-178AE00230D3_cropped.jpeg.\n",
    "\n",
    "\n",
    "\n",
    "Step 2: Building the model : The model should be able to predict whether or not an air conditioner is leaking based on the aerial imagery of the roof of a house. Priority 1 is class prediction of whether or not an air conditioner is present. Priority 2 is class prediction of whether or not an air conditioner is leaking. Priority 3 is the color of discolored air conditioners. The model could be trained from scratch, fine-tuned from a pre-trained model, use a pre-trained model as a feature extractor, or just be a model off-the-shelf.  \n",
    "\n",
    "Step 3: Model Evaluation : The model should be split into training, testing and validation sets. Hyperparameters may be tuned on the testing set. The model should be evaluated on the validation set. The model should account for class imbalance by weighting toward recall of minority classes.\n",
    "\n",
    "The model should be evaluated using the following metrics:\n",
    "- F1 Score (the loss function)\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- ROC AUC\n",
    "- Confusion Matrix\n",
    "- Classification Report\n",
    "- Precision-Recall Curve\n",
    "- ROC Curve\n",
    "- Loss Curve\n",
    "\n",
    "For this problem ...\n"
   ],
   "id": "5cb6dcce45a45dc4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**For this problem**, I'd recommend using a Convolutional Neural Network (CNN) to classify the images. CNNs are particularly well-suited for image classification tasks. You can use a pre-trained model as a feature extractor, or train a model from scratch. You can use any framework you like, but I'd recommend using **PyTorch**. You can use any additional libraries you like, but I'd recommend using scikit-learn for evaluation metrics.",
   "id": "5fad7527cafa4b57"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Given that we have 150 labeled images** and we are looking to classify the images into one of the following categories:\n",
    "- Air Conditioner Present\n",
    "- Air Conditioner Leaking\n",
    "- **Color of Leaking when Leaking is Present**\n",
    "\n",
    "This is a multi-class classification problem. We can use a CNN model to classify the images. \n",
    "\n",
    "We will be using a pre-trained model as a feature extractor. We will use the MobileNetV2 model, which is a small, low-latency model optimized for mobile devices. We will remove the top layer of the model, add a global average pooling layer, and add a dense layer with a softmax activation function to output the class probabilities.\n",
    "\n",
    "We will use the following evaluation metrics:\n",
    "- F1 Score (the loss function)\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- ROC AUC\n",
    "- Confusion Matrix\n",
    "- Classification Report\n",
    "- Precision-Recall Curve\n",
    "- ROC Curve\n",
    "- Loss Curve\n",
    "\n",
    "We will split the data into training, testing, and validation sets. We will tune hyperparameters on the testing set and evaluate the model on the validation set. We will account for class imbalance by weighting toward recall of minority classes.\n",
    "\n",
    "Let's get started!\n"
   ],
   "id": "4be433b36d97bcc4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T23:04:27.066596Z",
     "start_time": "2024-05-16T23:04:16.858236Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#**I need to install all the packages in this notebook**, so I will install them all at once\n",
    "# Path: image_classification.ipynb\n",
    "!pip install opencv-python\n",
    "!pip install torch\n",
    "!pip install torchvision\n",
    "!pip install tqdm\n",
    "!pip install seaborn\n",
    "!pip install scikit-learn\n",
    "!pip install pandas\n",
    "\n"
   ],
   "id": "7cf1f7c25656a6e4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /Users/jkerlin/PycharmProjects/satana/.venv/lib/python3.12/site-packages (4.9.0.80)\r\n",
      "Requirement already satisfied: numpy>=1.21.2 in /Users/jkerlin/PycharmProjects/satana/.venv/lib/python3.12/site-packages (from opencv-python) (1.26.4)\r\n",
      "Requirement already satisfied: torch in /Users/jkerlin/PycharmProjects/satana/.venv/lib/python3.12/site-packages (2.3.0)\r\n",
      "Requirement already satisfied: filelock in /Users/jkerlin/PycharmProjects/satana/.venv/lib/python3.12/site-packages (from torch) (3.14.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/jkerlin/PycharmProjects/satana/.venv/lib/python3.12/site-packages (from torch) (4.11.0)\r\n",
      "Requirement already satisfied: sympy in /Users/jkerlin/PycharmProjects/satana/.venv/lib/python3.12/site-packages (from torch) (1.12)\r\n",
      "Requirement already satisfied: networkx in /Users/jkerlin/PycharmProjects/satana/.venv/lib/python3.12/site-packages (from torch) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /Users/jkerlin/PycharmProjects/satana/.venv/lib/python3.12/site-packages (from torch) (3.1.3)\r\n",
      "Requirement already satisfied: fsspec in /Users/jkerlin/PycharmProjects/satana/.venv/lib/python3.12/site-packages (from torch) (2024.3.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/jkerlin/PycharmProjects/satana/.venv/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/jkerlin/PycharmProjects/satana/.venv/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\r\n",
      "Requirement already satisfied: torchvision in /Users/jkerlin/PycharmProjects/satana/.venv/lib/python3.12/site-packages (0.18.0)\r\n",
      "Requirement already satisfied: numpy in /Users/jkerlin/PycharmProjects/satana/.venv/lib/python3.12/site-packages (from torchvision) (1.26.4)\r\n",
      "Requirement already satisfied: torch==2.3.0 in /Users/jkerlin/PycharmProjects/satana/.venv/lib/python3.12/site-packages (from torchvision) (2.3.0)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/jkerlin/PycharmProjects/satana/.venv/lib/python3.12/site-packages (from torchvision) (10.3.0)\r\n",
      "Requirement already satisfied: filelock in /Users/jkerlin/PycharmProjects/satana/.venv/lib/python3.12/site-packages (from torch==2.3.0->torchvision) (3.14.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/jkerlin/PycharmProjects/satana/.venv/lib/python3.12/site-packages (from torch==2.3.0->torchvision) (4.11.0)\r\n",
      "Requirement already satisfied: sympy in /Users/jkerlin/PycharmProjects/satana/.venv/lib/python3.12/site-packages (from torch==2.3.0->torchvision) (1.12)\r\n",
      "Requirement already satisfied: networkx in /Users/jkerlin/PycharmProjects/satana/.venv/lib/python3.12/site-packages (from torch==2.3.0->torchvision) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /Users/jkerlin/PycharmProjects/satana/.venv/lib/python3.12/site-packages (from torch==2.3.0->torchvision) (3.1.3)\r\n",
      "Requirement already satisfied: fsspec in /Users/jkerlin/PycharmProjects/satana/.venv/lib/python3.12/site-packages (from torch==2.3.0->torchvision) (2024.3.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/jkerlin/PycharmProjects/satana/.venv/lib/python3.12/site-packages (from jinja2->torch==2.3.0->torchvision) (2.1.5)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/jkerlin/PycharmProjects/satana/.venv/lib/python3.12/site-packages (from sympy->torch==2.3.0->torchvision) (1.3.0)\r\n",
      "Requirement already satisfied: tqdm in /Users/jkerlin/PycharmProjects/satana/.venv/lib/python3.12/site-packages (4.66.4)\r\n",
      "Requirement already satisfied: seaborn in /Users/jkerlin/PycharmProjects/satana/.venv/lib/python3.12/site-packages (0.13.2)\r\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /Users/jkerlin/PycharmProjects/satana/.venv/lib/python3.12/site-packages (from seaborn) (1.26.4)\r\n",
      "Requirement already satisfied: pandas>=1.2 in /Users/jkerlin/PycharmProjects/satana/.venv/lib/python3.12/site-packages (from seaborn) (2.2.2)\r\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /Users/jkerlin/PycharmProjects/satana/.venv/lib/python3.12/site-packages (from seaborn) (3.8.4)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/jkerlin/PycharmProjects/satana/.venv/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.2.1)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/jkerlin/PycharmProjects/satana/.venv/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/jkerlin/PycharmProjects/satana/.venv/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.51.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/jkerlin/PycharmProjects/satana/.venv/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.5)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/jkerlin/PycharmProjects/satana/.venv/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.0)\r\n",
      "Requirement already satisfied: pillow>=8 in /Users/jkerlin/PycharmProjects/satana/.venv/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.3.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/jkerlin/PycharmProjects/satana/.venv/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/jkerlin/PycharmProjects/satana/.venv/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/jkerlin/PycharmProjects/satana/.venv/lib/python3.12/site-packages (from pandas>=1.2->seaborn) (2024.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/jkerlin/PycharmProjects/satana/.venv/lib/python3.12/site-packages (from pandas>=1.2->seaborn) (2024.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/jkerlin/PycharmProjects/satana/.venv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\r\n",
      "Collecting scikit-learn\r\n",
      "  Downloading scikit_learn-1.4.2-cp312-cp312-macosx_12_0_arm64.whl.metadata (11 kB)\r\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Users/jkerlin/PycharmProjects/satana/.venv/lib/python3.12/site-packages (from scikit-learn) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/jkerlin/PycharmProjects/satana/.venv/lib/python3.12/site-packages (from scikit-learn) (1.13.0)\r\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\r\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\r\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\r\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\r\n",
      "Downloading scikit_learn-1.4.2-cp312-cp312-macosx_12_0_arm64.whl (10.5 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.5/10.5 MB\u001B[0m \u001B[31m10.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m0:01\u001B[0m\r\n",
      "\u001B[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m301.8/301.8 kB\u001B[0m \u001B[31m9.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\r\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\r\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.4.2 threadpoolctl-3.5.0\r\n",
      "Requirement already satisfied: pandas in /Users/jkerlin/PycharmProjects/satana/.venv/lib/python3.12/site-packages (2.2.2)\r\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Users/jkerlin/PycharmProjects/satana/.venv/lib/python3.12/site-packages (from pandas) (1.26.4)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/jkerlin/PycharmProjects/satana/.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/jkerlin/PycharmProjects/satana/.venv/lib/python3.12/site-packages (from pandas) (2024.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/jkerlin/PycharmProjects/satana/.venv/lib/python3.12/site-packages (from pandas) (2024.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/jkerlin/PycharmProjects/satana/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\r\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T21:19:01.213747Z",
     "start_time": "2024-05-17T21:18:58.334107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Path: image_classification.ipynb\n",
    "# Load the necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, precision_recall_curve, f1_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.models import mobilenet_v2\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "import random\n"
   ],
   "id": "25f0617687e1533e",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1efa7b7731bf83d6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T21:19:04.673403Z",
     "start_time": "2024-05-17T21:19:04.642936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Path: image_classification.ipynb\n",
    "# Set the random seed for reproducibility\n",
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ],
   "id": "7910911528e11e53",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T21:19:11.045797Z",
     "start_time": "2024-05-17T21:19:10.999842Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "## Step 1: Preprocessing the data\n",
    "# Path: image_classification.ipynb\n",
    "# Load the data labels spreadsheet\n",
    "import re\n",
    "data_labels_path = '../data/labels/ac_150_labels.csv'\n",
    "data_labels = pd.read_csv(data_labels_path)\n",
    "\n",
    "data_labels['num_ac'] = data_labels['ac_present'].str.extract(r\"\\(\\s*(\\d+)\\s*\\)\").astype(float)\n",
    "\n",
    "\n",
    "\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "# Fill NaN values with 0\n",
    "data_labels['num_ac'] = data_labels['num_ac'].fillna(0).infer_objects(copy=False)\n",
    "\n",
    "def clean_string(s):\n",
    "    # Replace all non-alphabetic characters with an empty string\n",
    "    cleaned_string = re.sub(r'[^a-zA-Z]', '', s)\n",
    "    # Convert the string to lowercase\n",
    "    cleaned_string = cleaned_string.lower()\n",
    "    return cleaned_string\n",
    "\n",
    "# Apply the function to the the right columns column\n",
    "for col in ['ac_present', 'ac_leaking', 'ac_discolored']:\n",
    "    data_labels[col] = data_labels[col].astype(str)\n",
    "    data_labels[col] = data_labels[col].apply(clean_string)\n",
    "\n",
    "data_labels['black1hot'] = data_labels['ac_discolored'].str.contains('black').astype(int)\n",
    "\n",
    "data_labels['white1hot'] = data_labels['ac_discolored'].str.contains('white').astype(int)\n",
    "\n",
    "data_labels['present1hot'] = data_labels['ac_present'].str.contains('yes').astype(int)\n",
    "\n",
    "data_labels['leaking1hot'] = data_labels['ac_leaking'].str.contains('yes').astype(int)\n",
    "\n",
    "data_labels['present_unknown1hot'] = data_labels['ac_present'].str.contains('yes').astype(int)\n",
    "\n",
    "data_labels['leaking_unknown1hot'] = data_labels['ac_leaking'].str.contains('yes').astype(int)\n",
    "\n",
    "data_labels['color_unknown1hot'] = data_labels['ac_leaking'].str.contains('yes').astype(int)\n",
    "\n",
    "data_labels['ac_present'] = data_labels['ac_present'].astype(str)\n",
    "# Extract the numbers in brackets and convert them to integers\n",
    "assert data_labels.loc[data_labels['ac_present'] == 'yes', 'num_ac'].min() == 1\n",
    "\n",
    "data_labels.head()\n",
    "# # Remove any leading or trailing spaces\n",
    "# data_labels['ac_present'] = data_labels['ac_present'].str.strip()\n",
    "# data_labels.loc[data_labels['ac_present'] == 'Yes','num_ac'].unique()"
   ],
   "id": "82ccd8381c8dbf89",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   row_number                                            image_id  \\\n",
       "0            1  347ED694-833B-4F44-ABD9-8A26E9BF12A7_1_201_a.jpeg   \n",
       "1            2          56E8920C-C46B-4CC9-96CC-178AE00230D3.jpeg   \n",
       "2            3          0AD2D165-CE95-4F5C-BF88-524EA08A686E.jpeg   \n",
       "3            4          66B175F8-4DAC-4FBA-B8F5-C26711B0C71F.jpeg   \n",
       "4            5          55CE2555-D78E-49BE-8E89-476C586FEAE1.jpeg   \n",
       "\n",
       "                                           address ac_present ac_leaking  \\\n",
       "0       1330 W Willow St, Lafayette, LA 70506, USA        yes        yes   \n",
       "1  1110 N University Ave, Lafayette, LA 70506, USA        yes    unknown   \n",
       "2       106 Madeline Ave, Lafayette, LA 70501, USA         no         no   \n",
       "3     200 S Bienville St, Lafayette, LA 70501, USA        yes        yes   \n",
       "4      100 W St Louis St, Lafayette, LA 70506, USA         no         no   \n",
       "\n",
       "  ac_discolored                                           comments  num_ac  \\\n",
       "0    blackwhite                                                NaN     4.0   \n",
       "1       unknown  There is a black tone in all the roof parcel, ...     3.0   \n",
       "2            no                                                NaN     0.0   \n",
       "3         black                                                NaN     2.0   \n",
       "4            no                                                NaN     0.0   \n",
       "\n",
       "   black1hot  white1hot  present1hot  leaking1hot  present_unknown1hot  \\\n",
       "0          1          1            1            1                    1   \n",
       "1          0          0            1            0                    1   \n",
       "2          0          0            0            0                    0   \n",
       "3          1          0            1            1                    1   \n",
       "4          0          0            0            0                    0   \n",
       "\n",
       "   leaking_unknown1hot  color_unknown1hot  \n",
       "0                    1                  1  \n",
       "1                    0                  0  \n",
       "2                    0                  0  \n",
       "3                    1                  1  \n",
       "4                    0                  0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_number</th>\n",
       "      <th>image_id</th>\n",
       "      <th>address</th>\n",
       "      <th>ac_present</th>\n",
       "      <th>ac_leaking</th>\n",
       "      <th>ac_discolored</th>\n",
       "      <th>comments</th>\n",
       "      <th>num_ac</th>\n",
       "      <th>black1hot</th>\n",
       "      <th>white1hot</th>\n",
       "      <th>present1hot</th>\n",
       "      <th>leaking1hot</th>\n",
       "      <th>present_unknown1hot</th>\n",
       "      <th>leaking_unknown1hot</th>\n",
       "      <th>color_unknown1hot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>347ED694-833B-4F44-ABD9-8A26E9BF12A7_1_201_a.jpeg</td>\n",
       "      <td>1330 W Willow St, Lafayette, LA 70506, USA</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>blackwhite</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>56E8920C-C46B-4CC9-96CC-178AE00230D3.jpeg</td>\n",
       "      <td>1110 N University Ave, Lafayette, LA 70506, USA</td>\n",
       "      <td>yes</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>There is a black tone in all the roof parcel, ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0AD2D165-CE95-4F5C-BF88-524EA08A686E.jpeg</td>\n",
       "      <td>106 Madeline Ave, Lafayette, LA 70501, USA</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>66B175F8-4DAC-4FBA-B8F5-C26711B0C71F.jpeg</td>\n",
       "      <td>200 S Bienville St, Lafayette, LA 70501, USA</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>black</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>55CE2555-D78E-49BE-8E89-476C586FEAE1.jpeg</td>\n",
       "      <td>100 W St Louis St, Lafayette, LA 70506, USA</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T23:12:26.877317Z",
     "start_time": "2024-05-17T23:12:26.870873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def crop_images_by_box(params):\n",
    "    for image_id in params['image_ids']:  \n",
    "        image_path = os.path.join(params['images_folder'], image_id)\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Find the orange bounding box in the image\n",
    "        hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "        if params['hsv_range'] is not None:\n",
    "            lower_orange = np.array(params['hsv_range'][0])\n",
    "            upper_orange = np.array(params['hsv_range'][1])\n",
    "            mask = cv2.inRange(hsv, lower_orange, upper_orange)\n",
    "        else:\n",
    "            mask = cv2.inRange(hsv, np.array([0,0,0]),np.array([180,255,255])) #no filter\n",
    "    \n",
    "        \n",
    "        # Perform dilation on the mask to close small gaps\n",
    "        if params['kernal'] is not None:\n",
    "            kernel = params['kernal']\n",
    "            mask = cv2.dilate(mask, kernel, iterations=2)\n",
    "        \n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        # # Function to find the contour with the highest perimeter-to-area ratio\n",
    "        # def find_hollow_box(contours):\n",
    "        #     hollow_contour = None\n",
    "        #     for contour in contours:\n",
    "        #         area = cv2.contourArea(contour)\n",
    "        #         perimeter = cv2.arcLength(contour, True)\n",
    "        #         if area > params['min_area']:\n",
    "        #             ratio = perimeter / area\n",
    "        #             if ratio > params['min_ratio'] and perimeter > params['min_perimeter'] and perimeter < params['max_perimeter']:\n",
    "        #                 hollow_contour = contour\n",
    "        #                 print('ratio:' + str(ratio) + ' perimeter:' + str(perimeter) + ' area:' + str(area) + ' of image ' + image_id)\n",
    "        #                 return hollow_contour   \n",
    "        #     return None\n",
    "        # # Find the contour that best matches a hollow box\n",
    "        # contours = find_hollow_box(contours)\n",
    "        if contours is None:\n",
    "            print(f'No bounding box found for image {image_id}')\n",
    "            continue\n",
    "        else:\n",
    "            # Find the largest rectangular contour\n",
    "            largest_contour = max(contours, key=cv2.contourArea)\n",
    "            \n",
    "            # Get the bounding rectangle of the largest rectangular contour\n",
    "            x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "            # Crop the image around the bounding rectangle\n",
    "            cropped_image = image[y:y+h, x:x+w]\n",
    "            \n",
    "            # Save the cropped image\n",
    "            cropped_image_path = os.path.join(params['cropped_images_folder'], image_id.replace('.jpeg', '_cropped.jpeg'))\n",
    "            cv2.imwrite(cropped_image_path, cv2.cvtColor(cropped_image, cv2.COLOR_RGB2BGR))\n",
    "            print(f'Image {image_id} cropped and saved to {cropped_image_path}')\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ],
   "id": "b2816ee94b3da27",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T03:53:23.025537Z",
     "start_time": "2024-05-18T03:53:23.020353Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def crop_images_by_box(params):\n",
    "    for image_id in params['image_ids']:  \n",
    "        image_path = os.path.join(params['images_folder'], image_id)\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Find the orange bounding box in the image\n",
    "        hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "        if params['hsv_range'] is not None:\n",
    "            lower_orange = np.array(params['hsv_range'][0])\n",
    "            upper_orange = np.array(params['hsv_range'][1])\n",
    "            mask = cv2.inRange(hsv, lower_orange, upper_orange)\n",
    "        else:\n",
    "            mask = cv2.inRange(hsv, np.array([0,0,0]),np.array([180,255,255])) #no filter\n",
    "    \n",
    "        \n",
    "        # Perform dilation on the mask to close small gaps\n",
    "        if params['kernal'] is not None:\n",
    "            kernel = params['kernal']\n",
    "            mask = cv2.dilate(mask, kernel, iterations=2)\n",
    "        \n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        # Function to find the contour with the highest perimeter-to-area ratio\n",
    "        def find_hollow_box(contours, mask, params):\n",
    "            hollow_contours = []\n",
    "            for contour in contours:\n",
    "                # Get the bounding rectangle of the largest rectangular contour\n",
    "                x, y, w, h = cv2.boundingRect(contour)\n",
    "                # Crop the mask around the bounding rectangle\n",
    "                cropped_mask = mask[y:y+h, x:x+w]\n",
    "                mask_area = cv2.countNonZero(cropped_mask)\n",
    "                bb_area = w*h\n",
    "    \n",
    "                if mask_area > 0:\n",
    "                    ratio = bb_area / mask_area\n",
    "                    if ratio > params['min_ratio'] and bb_area > params['min_bb_area'] and bb_area < params['max_bb_area']:\n",
    "                        hollow_contours.append(contour)\n",
    "                        #print('ratio:' + str(ratio) + ' bb_area:' + str(bb_area) + ' mask_area:' + str(mask_area) + ' of image ' + image_id)\n",
    "                         \n",
    "            return hollow_contours  \n",
    "        # Find the contour that best matches a hollow box\n",
    "        contours = find_hollow_box(contours, mask, params)\n",
    "        if contours == []:\n",
    "            print(f'No bounding box found for image {image_id}')\n",
    "            continue\n",
    "        else:\n",
    "            # Find the largest rectangular contour\n",
    "            largest_contour = max(contours, key=cv2.contourArea)\n",
    "            \n",
    "            # Get the bounding rectangle of the largest rectangular contour\n",
    "            x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "            # Crop the image around the bounding rectangle\n",
    "            cropped_image = image[y:y+h, x:x+w]\n",
    "    \n",
    "            # Save the cropped image\n",
    "            cropped_image_path = os.path.join(params['cropped_images_folder'], image_id.replace('.jpeg', '_cropped.jpeg'))\n",
    "            cv2.imwrite(cropped_image_path, cv2.cvtColor(cropped_image, cv2.COLOR_RGB2BGR))\n",
    "            \n",
    "            #print(f'Image {image_id} cropped and saved to {cropped_image_path}')\n",
    "    return image,cropped_image,mask"
   ],
   "id": "1ac1fbd5cb643e8",
   "outputs": [],
   "execution_count": 114
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T03:52:48.884409Z",
     "start_time": "2024-05-18T03:52:48.882252Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "53d1738df67a6b85",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'347ED694-833B-4F44-ABD9-8A26E9BF12A7_1_201_a.jpeg'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 113
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T04:20:26.299694Z",
     "start_time": "2024-05-18T04:20:24.020188Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Load the images and crop them around the orange bounding box\n",
    "# Path: image_classification.ipynb\n",
    "images_folder = '../data/images/ac_150_images/'\n",
    "cropped_images_folder = '../data/images/ac_150_images_cropped/'\n",
    "os.makedirs(cropped_images_folder, exist_ok=True)\n",
    "\n",
    "params = {}\n",
    "params['image_ids'] = data_labels['image_id']\n",
    "#params['image_ids'] = np.array(['347ED694-833B-4F44-ABD9-8A26E9BF12A7_1_201_a.jpeg'])\n",
    "params['images_folder'] = images_folder\n",
    "params['cropped_images_folder'] = cropped_images_folder\n",
    "params['hsv_range'] = [[8, 120, 200], [12, 255, 255]] #filter for a specific color (tested for 100% capture of 150 examples\n",
    "params['kernal'] = np.ones((20,20), np.uint8)\n",
    "params['min_bb_area'] = 1\n",
    "params['max_bb_area'] = 1000000000\n",
    "params['min_ratio'] = 1.0\n",
    "\n",
    "image, cropped_image, mask = crop_images_by_box(params)"
   ],
   "id": "a8dbdfe942628309",
   "outputs": [],
   "execution_count": 218
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T04:19:58.394072Z",
     "start_time": "2024-05-18T04:19:58.246890Z"
    }
   },
   "cell_type": "code",
   "source": "plt.imshow(mask)",
   "id": "d5cbed6fb8c099a9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x3247cb2f0>"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 216
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T04:23:20.048945Z",
     "start_time": "2024-05-18T04:23:20.045579Z"
    }
   },
   "cell_type": "code",
   "source": [
    "    \n",
    "## Step 2: Building the model\n",
    "# Path: image_classification.ipynb\n",
    "# Define the dataset class\n",
    "class ACDataset(Dataset):\n",
    "    def __init__(self, data_labels, images_folder, transform=None):\n",
    "        self.data_labels = data_labels\n",
    "        self.images_folder = images_folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.data_labels.iloc[idx]['image_id']\n",
    "        image_path = os.path.join(self.images_folder, image_id)\n",
    "        image = Image.open(image_path)\n",
    "        ac_present = self.data_labels.iloc[idx]['ac_present']\n",
    "        ac_leaking = self.data_labels.iloc[idx]['ac_leaking']\n",
    "        num_ac = self.data_labels.iloc[idx]['num_ac']\n",
    "        ac_discolored = self.data_labels.iloc[idx]['ac_discolored']\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, ac_present, ac_leaking, num_ac, ac_discolored"
   ],
   "id": "f2650b395d97783",
   "outputs": [],
   "execution_count": 219
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T04:24:11.000521Z",
     "start_time": "2024-05-18T04:24:10.997370Z"
    }
   },
   "cell_type": "code",
   "source": "ACDataset(data_labels, images_folder, transform=None)",
   "id": "cfe8b0dfa9fc1d25",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ACDataset at 0x3332c5940>"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 220
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T04:27:24.529781Z",
     "start_time": "2024-05-18T04:27:24.522466Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create the dataset\n",
    "dataset = ACDataset(data_labels, cropped_images_folder, transform=transform)"
   ],
   "id": "29e60dd518a1b0e9",
   "outputs": [],
   "execution_count": 222
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T04:27:26.410588Z",
     "start_time": "2024-05-18T04:27:26.401810Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Split the dataset into training, testing, and validation sets\n",
    "\n",
    "train_size = 0.8\n",
    "test_size = 0.1\n",
    "val_size = 0.1\n",
    "num_samples = len(dataset)\n",
    "\n",
    "indices = np.arange(num_samples)\n",
    "train_indices, test_val_indices = train_test_split(indices, test_size=test_size+val_size, random_state=random_seed)\n",
    "test_val_indices = np.array(test_val_indices)\n",
    "test_indices, val_indices = train_test_split(test_val_indices, test_size=val_size/(test_size+val_size), random_state=random_seed)\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "test_dataset = torch.utils.data.Subset(dataset, test_indices)\n",
    "val_dataset = torch.utils.data.Subset(dataset, val_indices)\n"
   ],
   "id": "a15a15b7c647ef80",
   "outputs": [],
   "execution_count": 223
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T04:28:01.553351Z",
     "start_time": "2024-05-18T04:28:01.547258Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the dataloaders\n",
    "batch_size = 16\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "id": "ad51d32a6aacf8e5",
   "outputs": [],
   "execution_count": 225
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T04:40:43.075021Z",
     "start_time": "2024-05-18T04:40:43.002496Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the pretrained MobileNetV2 model from torchvision\n",
    "model_folder = '../data/models/'\n",
    "model = mobilenet_v2()\n",
    "checkpoint = torch.load(model_folder + 'mobilenet_v2-b0353104.pth')\n",
    "\n",
    "# Load the weights from the checkpoint into the model\n",
    "model.load_state_dict(checkpoint)\n"
   ],
   "id": "30506d4cdd803b3c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 235
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T04:40:48.543488Z",
     "start_time": "2024-05-18T04:40:48.541266Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Retrain the model from the last layer\n",
    "model.classifier[1] = nn.Linear(1280, 2)\n"
   ],
   "id": "18da0fafdff65993",
   "outputs": [],
   "execution_count": 236
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T04:40:50.182115Z",
     "start_time": "2024-05-18T04:40:50.177876Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ],
   "id": "3c54258de131e882",
   "outputs": [],
   "execution_count": 237
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T04:40:52.485460Z",
     "start_time": "2024-05-18T04:40:52.051153Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train the model\n",
    "num_epochs = 10\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images, ac_present, ac_leaking, num_ac, ac_discolored in tqdm(train_dataloader):\n",
    "        images = images.to(device)\n",
    "        ac_present = ac_present.to(device)\n",
    "        ac_leaking = ac_leaking.to(device)\n",
    "        num_ac = num_ac.to(device)\n",
    "        ac_discolored = ac_discolored.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, ac_present)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_losses.append(train_loss / len(train_dataloader))\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_losses[-1]}')\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    for images, ac_present, ac_leaking, num_ac, ac_discolored in tqdm(val_dataloader):\n",
    "        images = images.to(device)\n",
    "        ac_present = ac_present.to(device)\n",
    "        ac_leaking = ac_leaking.to(device)\n",
    "        num_ac = num_ac.to(device)\n",
    "        ac_discolored = ac_discolored.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, ac_present)\n",
    "        val_loss += loss.item()\n",
    "\n",
    "    val_losses.append(val_loss / len(val_dataloader))\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_losses[-1]}')"
   ],
   "id": "626c33f51689b522",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/jkerlin/PycharmProjects/satana/data/images/ac_150_images_cropped/6898DE7F-74F6-42C7-80DC-6DA4D1319300.jpeg'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[238], line 12\u001B[0m\n\u001B[1;32m     10\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[1;32m     11\u001B[0m train_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[0;32m---> 12\u001B[0m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mimages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mac_present\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mac_leaking\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_ac\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mac_discolored\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtqdm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[43m    \u001B[49m\u001B[43mimages\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mimages\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[43mac_present\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mac_present\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/satana/.venv/lib/python3.12/site-packages/tqdm/std.py:1181\u001B[0m, in \u001B[0;36mtqdm.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1178\u001B[0m time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_time\n\u001B[1;32m   1180\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1181\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43miterable\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m   1182\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01myield\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\n\u001B[1;32m   1183\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Update and possibly print the progressbar.\u001B[39;49;00m\n\u001B[1;32m   1184\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;49;00m\n",
      "File \u001B[0;32m~/PycharmProjects/satana/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:631\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    628\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    629\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    630\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 631\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    632\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    633\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    635\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/PycharmProjects/satana/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:675\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    673\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    674\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 675\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    676\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    677\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/PycharmProjects/satana/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:49\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_collation:\n\u001B[1;32m     48\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__getitems__\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__:\n\u001B[0;32m---> 49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__getitems__\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpossibly_batched_index\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n",
      "File \u001B[0;32m~/PycharmProjects/satana/.venv/lib/python3.12/site-packages/torch/utils/data/dataset.py:419\u001B[0m, in \u001B[0;36mSubset.__getitems__\u001B[0;34m(self, indices)\u001B[0m\n\u001B[1;32m    417\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__([\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindices[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices])  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[1;32m    418\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 419\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindices\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices]\n",
      "Cell \u001B[0;32mIn[219], line 16\u001B[0m, in \u001B[0;36mACDataset.__getitem__\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m     14\u001B[0m image_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata_labels\u001B[38;5;241m.\u001B[39miloc[idx][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimage_id\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m     15\u001B[0m image_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimages_folder, image_id)\n\u001B[0;32m---> 16\u001B[0m image \u001B[38;5;241m=\u001B[39m \u001B[43mImage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     17\u001B[0m ac_present \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata_labels\u001B[38;5;241m.\u001B[39miloc[idx][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mac_present\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m     18\u001B[0m ac_leaking \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata_labels\u001B[38;5;241m.\u001B[39miloc[idx][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mac_leaking\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "File \u001B[0;32m~/PycharmProjects/satana/.venv/lib/python3.12/site-packages/PIL/Image.py:3277\u001B[0m, in \u001B[0;36mopen\u001B[0;34m(fp, mode, formats)\u001B[0m\n\u001B[1;32m   3274\u001B[0m     filename \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mrealpath(os\u001B[38;5;241m.\u001B[39mfspath(fp))\n\u001B[1;32m   3276\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m filename:\n\u001B[0;32m-> 3277\u001B[0m     fp \u001B[38;5;241m=\u001B[39m \u001B[43mbuiltins\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3278\u001B[0m     exclusive_fp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   3280\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/Users/jkerlin/PycharmProjects/satana/data/images/ac_150_images_cropped/6898DE7F-74F6-42C7-80DC-6DA4D1319300.jpeg'"
     ]
    }
   ],
   "execution_count": 238
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Evaluate the model\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for images, ac_present, ac_leaking, num_ac, ac_discolored in tqdm(test_dataloader):\n",
    "    images = images.to(device)\n",
    "    ac_present = ac_present.to(device)\n",
    "    ac_leaking = ac_leaking.to(device)\n",
    "    num_ac = num_ac.to(device)\n",
    "    ac_discol"
   ],
   "id": "b263c60912350db5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T04:33:54.028131Z",
     "start_time": "2024-05-18T04:33:54.024984Z"
    }
   },
   "cell_type": "code",
   "source": "dir(model)",
   "id": "3f4eec03ccc40e9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_backward_hooks',\n",
       " '_backward_pre_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_compiled_call_impl',\n",
       " '_forward_hooks',\n",
       " '_forward_hooks_always_called',\n",
       " '_forward_hooks_with_kwargs',\n",
       " '_forward_impl',\n",
       " '_forward_pre_hooks',\n",
       " '_forward_pre_hooks_with_kwargs',\n",
       " '_get_backward_hooks',\n",
       " '_get_backward_pre_hooks',\n",
       " '_get_name',\n",
       " '_is_full_backward_hook',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_post_hooks',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_state_dict_pre_hooks',\n",
       " '_version',\n",
       " '_wrapped_call_impl',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'call_super_init',\n",
       " 'children',\n",
       " 'classifier',\n",
       " 'compile',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'features',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'get_buffer',\n",
       " 'get_extra_state',\n",
       " 'get_parameter',\n",
       " 'get_submodule',\n",
       " 'half',\n",
       " 'ipu',\n",
       " 'last_channel',\n",
       " 'load_state_dict',\n",
       " 'modules',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'parameters',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_full_backward_pre_hook',\n",
       " 'register_load_state_dict_post_hook',\n",
       " 'register_module',\n",
       " 'register_parameter',\n",
       " 'register_state_dict_pre_hook',\n",
       " 'requires_grad_',\n",
       " 'set_extra_state',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'to_empty',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 231
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ],
   "id": "4cec66ef0efa609d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Train the model\n",
    "num_epochs = 10\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images, ac_present, ac_leaking, num_ac, ac_discolored in tqdm(train_dataloader):\n",
    "        images = images.to(device)\n",
    "        ac_present = ac_present.to(device)\n",
    "        ac_leaking = ac_leaking.to(device)\n",
    "        num_ac = num_ac.to(device)\n",
    "        ac_discolored = ac_discolored.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, ac_present)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_losses.append(train_loss / len(train_dataloader))\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_losses[-1]}')\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    for images, ac_present, ac_leaking, num_ac, ac_discolored in tqdm(val_dataloader):\n",
    "        images = images.to(device)\n",
    "        ac_present = ac_present.to(device)\n",
    "        ac_leaking = ac_leaking.to(device)\n",
    "        num_ac = num_ac.to(device)\n",
    "        ac_discolored = ac_discolored.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, ac_present)\n",
    "        val_loss += loss.item()\n",
    "\n",
    "    val_losses.append(val_loss / len(val_dataloader))\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_losses[-1]}')"
   ],
   "id": "a2b465d65b44aee1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Evaluate the model\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for images"
   ],
   "id": "855e28715bed4c13"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
