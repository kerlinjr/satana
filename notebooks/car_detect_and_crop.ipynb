{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_image\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection \n",
    "import os\n",
    "from skimage import exposure\n",
    "from torchvision.transforms.functional import to_pil_image, to_tensor\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "model_id = \"IDEA-Research/grounding-dino-tiny\"\n",
    "device = \"mps\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = AutoModelForZeroShotObjectDetection.from_pretrained(model_id).to(device)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "image_type = \"car\"\n",
    "root_folder = \"/Users/jkerlin/PycharmProjects/satana/data/images/\"\n",
    "input_image_folder = root_folder + \"gold_standard/\"\n",
    "output_image_folder = root_folder + \"gold_standard_crop/\"\n",
    "show_example = False\n",
    "\n",
    "\n",
    "if image_type == 'car':\n",
    "    text = \"individual cars from an satellite view.\""
   ],
   "id": "3420a56e4dd8aef4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "61b40504d77a118f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "# Check input folder for valid images\n",
    "valid_extensions = [\".jpg\", \".jpeg\", \".png\", \".bmp\", \".gif\"]\n",
    "image_list = []\n",
    "for file in os.listdir(input_image_folder):\n",
    "    # Get the file extension\n",
    "    _, extension = os.path.splitext(file)\n",
    "    \n",
    "    # If the file is an image, process it\n",
    "    if extension.lower() in valid_extensions:\n",
    "        image_list.append(file)\n",
    "\n",
    "#loop through all the images in the input_image_folder\n",
    "        \n",
    "for i,image_file in enumerate(image_list):\n",
    "\n",
    "    image_filepath = input_image_folder + image_file\n",
    "    image = Image.open(image_filepath).convert(\"RGB\")\n",
    "    \n",
    "    \n",
    "\n",
    "    # # Convert the image to a numpy array\n",
    "    # image_np = np.array(image)\n",
    "    # \n",
    "    # # Apply contrast stretching\n",
    "    # p_low, p_high = np.percentile(image_np, (0, 20))\n",
    "    # image_rescale = exposure.rescale_intensity(image_np, in_range=(p_low, p_high))\n",
    "    # \n",
    "    # # Convert the rescaled image back to a PIL Image\n",
    "    # image = Image.fromarray(image_rescale)\n",
    "\n",
    "\n",
    "\n",
    "    inputs = processor(images=image, text=text, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    results = processor.post_process_grounded_object_detection(\n",
    "        outputs,\n",
    "        inputs.input_ids,\n",
    "        box_threshold=0.15,\n",
    "        text_threshold=0.3,\n",
    "        target_sizes=[image.size[::-1]]\n",
    "    )\n",
    "\n",
    "    # Define your size criteria\n",
    "    min_dim = 15\n",
    "    max_dim = 150\n",
    "\n",
    "    # Filter the bounding boxes\n",
    "    filtered_boxes = []\n",
    "    box_area = []\n",
    "    for box in results[0][\"boxes\"].tolist():\n",
    "        width = box[2] - box[0]\n",
    "        height = box[3] - box[1]\n",
    "        if width > min_dim and height > min_dim and width < max_dim and height < max_dim:\n",
    "            filtered_boxes.append(box)\n",
    "            #box_area.append(width * height)\n",
    "\n",
    "    if show_example == True and i == 0:\n",
    "        tensor_boxes = torch.tensor(filtered_boxes) \n",
    "        tensor_image = transforms.ToTensor()(image)\n",
    "        # Convert the Tensor back to uint8\n",
    "        tensor_image = tensor_image.mul(255).byte()\n",
    "    \n",
    "        bbox_img = draw_bounding_boxes(tensor_image, tensor_boxes, width = 3, colors = \"red\")\n",
    "        bbox_img = torchvision.transforms.ToPILImage()(bbox_img)\n",
    "    \n",
    "        bbox_img.show()\n",
    "        break\n",
    "    else:\n",
    "        for i, box in enumerate(filtered_boxes):\n",
    "            # PIL's crop function takes a tuple of (left, upper, right, lower) pixel coordinates\n",
    "            # Here, we assume that each box is a list of [xmin, ymin, xmax, ymax]\n",
    "            cropped_image = image.crop((box[0], box[1], box[2], box[3]))\n",
    "    \n",
    "            # Save the cropped image as a jpg file\n",
    "            cropped_image.save(output_image_folder + image_file[:-4] + \"_crop_\" + str(i) + \".jpg\")\n",
    "            \n",
    "\n"
   ],
   "id": "eff98111d2b83830"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
